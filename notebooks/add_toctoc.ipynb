{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put this at the beginning of a vscode notebook\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# make rpnpy visible\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = (\n",
    "    \"/fs/ssm/eccc/mrd/rpn/libs/19.6.0/ubuntu-18.04-skylake-64/lib/ubuntu-18.04-skylake-64/intel-19.0.3.199:/fs/ssm/eccc/mrd/rpn/vgrid/6.5.0/ubuntu-18.04-skylake-64/lib/ubuntu-18.04-skylake-64/intel-19.0.3.199:/fs/ssm/eccc/mrd/rpn/utils/19.6.0/ubuntu-18.04-skylake-64/lib:/fs/ssm/eccc/mrd/rpn/utils/19.6.0/ubuntu-18.04-amd64-64/lib:/fs/ssm/hpco/exp/openmpi/hpcx-core-2.4.0/hpcx-core_2.4.0-gcc-mofed-4.6-1_ubuntu-18.04-amd64-64/x/ucx/lib:/fs/ssm/hpco/exp/openmpi/hpcx-core-2.4.0/hpcx-core_2.4.0-gcc-mofed-4.6-1_ubuntu-18.04-amd64-64/x/sharp/lib:/fs/ssm/hpco/exp/openmpi/hpcx-core-2.4.0/hpcx-core_2.4.0-gcc-mofed-4.6-1_ubuntu-18.04-amd64-64/x/hcoll/lib:/fs/ssm/hpco/exp/openmpi/openmpi-3.1.2--hpcx-2.4.0-mofed-4.6--intel-19.0.3.199/ubuntu-18.04-amd64-64/lib:/fs/ssm/main/opt/intelcomp/intelpsxe-cluster-19.0.3.199/intelpsxe-cluster_19.0.3.199_multi/compilers_and_libraries_2019.3.199/linux/compiler/lib/intel64_lin:/fs/ssm/main/opt/intelcomp/intelpsxe-cluster-19.0.3.199/intelpsxe-cluster_19.0.3.199_multi/compilers_and_libraries_2019.3.199/linux/ipp/lib/intel64:/fs/ssm/main/opt/intelcomp/intelpsxe-cluster-19.0.3.199/intelpsxe-cluster_19.0.3.199_multi/compilers_and_libraries_2019.3.199/linux/mkl/lib/intel64_lin:/fs/ssm/main/opt/intelcomp/intelpsxe-cluster-19.0.3.199/intelpsxe-cluster_19.0.3.199_multi/compilers_and_libraries_2019.3.199/linux/tbb/lib/intel64/gcc4.7:/fs/ssm/main/opt/intelcomp/intelpsxe-cluster-19.0.3.199/intelpsxe-cluster_19.0.3.199_multi/debugger_2019/libipt/intel64/lib:/fs/ssm/main/opt/intelcomp/intelpsxe-cluster-19.0.3.199/intelpsxe-cluster_19.0.3.199_multi/compilers_and_libraries_2019.3.199/linux/daal/lib/intel64_lin:/fs/ssm/main/opt/intelcomp/intelpsxe-cluster-19.0.3.199/intelpsxe-cluster_19.0.3.199_multi/compilers_and_libraries_2019.3.199/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/opt/mellanox/mxm/lib\"\n",
    ")\n",
    "sys.path.append(\"/fs/ssm/eccc/mrd/rpn/MIG/ENV/d/rpnpy/rpnpy_2.1.2/rpnpy_2.1.2_all/lib\")\n",
    "sys.path.append(\"/fs/ssm/eccc/mrd/rpn/libs/19.6.0/ubuntu-18.04-skylake-64/lib/ubuntu-18.04-skylake-64/intel-19.0.3.199\")\n",
    "sys.path.append(\"/fs/ssm/eccc/mrd/rpn/vgrid/6.5.0/ubuntu-18.04-skylake-64/lib/ubuntu-18.04-skylake-64/intel-19.0.3.199\")\n",
    "sys.path.append(\"/fs/ssm/eccc/cmd/cmda/libs/19.6.0/intel-19.0.3.199/ubuntu-18.04-skylake-64/lib\")\n",
    "sys.path.append(\"/fs/ssm/eccc/mrd/rpn/utils/19.6.0/env-python_1.3.0_all/share/Python/PythonAllRevs\")\n",
    "# make ci_fstcomp visible\n",
    "sys.path.append(\"/fs/ssm/eccc/cmd/cmds/apps/ci_fstcomp/1.0.8/all/lib/python\")\n",
    "sys.path.append(\"/fs/ssm/eccc/crd/ccmr/EC-CAS/master/fstd2nc_0.20211202.0-a1_all/lib/python\")\n",
    "# check this path points on fstpy\n",
    "sys.path.append(str(pathlib.Path(os.getcwd()).parent))\n",
    "import fstpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nomvar</th>\n",
       "      <th>typvar</th>\n",
       "      <th>etiket</th>\n",
       "      <th>ni</th>\n",
       "      <th>nj</th>\n",
       "      <th>nk</th>\n",
       "      <th>dateo</th>\n",
       "      <th>ip1</th>\n",
       "      <th>ip2</th>\n",
       "      <th>ip3</th>\n",
       "      <th>...</th>\n",
       "      <th>nbits</th>\n",
       "      <th>grtyp</th>\n",
       "      <th>ig1</th>\n",
       "      <th>ig2</th>\n",
       "      <th>ig3</th>\n",
       "      <th>ig4</th>\n",
       "      <th>datev</th>\n",
       "      <th>grid</th>\n",
       "      <th>date_of_observation</th>\n",
       "      <th>date_of_observation_Canada_Eastern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HU</td>\n",
       "      <td>P</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>1108</td>\n",
       "      <td>1082</td>\n",
       "      <td>1</td>\n",
       "      <td>442998800</td>\n",
       "      <td>95529009</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>Z</td>\n",
       "      <td>33792</td>\n",
       "      <td>77761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>443004200</td>\n",
       "      <td>3379277761</td>\n",
       "      <td>2020-07-14 12:00:00</td>\n",
       "      <td>2020-07-14 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HU</td>\n",
       "      <td>P</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>1108</td>\n",
       "      <td>1082</td>\n",
       "      <td>1</td>\n",
       "      <td>442998800</td>\n",
       "      <td>97351772</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>Z</td>\n",
       "      <td>33792</td>\n",
       "      <td>77761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>443004200</td>\n",
       "      <td>3379277761</td>\n",
       "      <td>2020-07-14 12:00:00</td>\n",
       "      <td>2020-07-14 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GZ</td>\n",
       "      <td>P</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>1108</td>\n",
       "      <td>1082</td>\n",
       "      <td>1</td>\n",
       "      <td>442998800</td>\n",
       "      <td>95364364</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>Z</td>\n",
       "      <td>33792</td>\n",
       "      <td>77761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>443004200</td>\n",
       "      <td>3379277761</td>\n",
       "      <td>2020-07-14 12:00:00</td>\n",
       "      <td>2020-07-14 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GZ</td>\n",
       "      <td>P</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>1108</td>\n",
       "      <td>1082</td>\n",
       "      <td>1</td>\n",
       "      <td>442998800</td>\n",
       "      <td>95357866</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>Z</td>\n",
       "      <td>33792</td>\n",
       "      <td>77761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>443004200</td>\n",
       "      <td>3379277761</td>\n",
       "      <td>2020-07-14 12:00:00</td>\n",
       "      <td>2020-07-14 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TT</td>\n",
       "      <td>P</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>1108</td>\n",
       "      <td>1082</td>\n",
       "      <td>1</td>\n",
       "      <td>442998800</td>\n",
       "      <td>95178882</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>Z</td>\n",
       "      <td>33792</td>\n",
       "      <td>77761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>443004200</td>\n",
       "      <td>3379277761</td>\n",
       "      <td>2020-07-14 12:00:00</td>\n",
       "      <td>2020-07-14 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>HF</td>\n",
       "      <td>P</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>1104</td>\n",
       "      <td>1078</td>\n",
       "      <td>1</td>\n",
       "      <td>442998800</td>\n",
       "      <td>60168832</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Z</td>\n",
       "      <td>35132</td>\n",
       "      <td>56748</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>443004200</td>\n",
       "      <td>3513256748</td>\n",
       "      <td>2020-07-14 12:00:00</td>\n",
       "      <td>2020-07-14 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>FATB</td>\n",
       "      <td>P</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>1104</td>\n",
       "      <td>1078</td>\n",
       "      <td>1</td>\n",
       "      <td>442998800</td>\n",
       "      <td>60368832</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>Z</td>\n",
       "      <td>35132</td>\n",
       "      <td>56748</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>443004200</td>\n",
       "      <td>3513256748</td>\n",
       "      <td>2020-07-14 12:00:00</td>\n",
       "      <td>2020-07-14 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>&gt;&gt;</td>\n",
       "      <td>X</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>1104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>442998800</td>\n",
       "      <td>35132</td>\n",
       "      <td>56748</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>E</td>\n",
       "      <td>1470</td>\n",
       "      <td>560</td>\n",
       "      <td>54400</td>\n",
       "      <td>46560</td>\n",
       "      <td>442998800</td>\n",
       "      <td>3513256748</td>\n",
       "      <td>2020-07-14 12:00:00</td>\n",
       "      <td>2020-07-14 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>^^</td>\n",
       "      <td>X</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>1</td>\n",
       "      <td>1078</td>\n",
       "      <td>1</td>\n",
       "      <td>442998800</td>\n",
       "      <td>35132</td>\n",
       "      <td>56748</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>E</td>\n",
       "      <td>1470</td>\n",
       "      <td>560</td>\n",
       "      <td>54400</td>\n",
       "      <td>46560</td>\n",
       "      <td>442998800</td>\n",
       "      <td>3513256748</td>\n",
       "      <td>2020-07-14 12:00:00</td>\n",
       "      <td>2020-07-14 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>!!</td>\n",
       "      <td>X</td>\n",
       "      <td>R1_V710_N</td>\n",
       "      <td>3</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35132</td>\n",
       "      <td>56748</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>X</td>\n",
       "      <td>5005</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>3513256748</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1874 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     nomvar typvar     etiket    ni    nj  nk      dateo       ip1    ip2  \\\n",
       "0        HU      P  R1_V710_N  1108  1082   1  442998800  95529009      6   \n",
       "1        HU      P  R1_V710_N  1108  1082   1  442998800  97351772      6   \n",
       "2        GZ      P  R1_V710_N  1108  1082   1  442998800  95364364      6   \n",
       "3        GZ      P  R1_V710_N  1108  1082   1  442998800  95357866      6   \n",
       "4        TT      P  R1_V710_N  1108  1082   1  442998800  95178882      6   \n",
       "...     ...    ...        ...   ...   ...  ..        ...       ...    ...   \n",
       "1869     HF      P  R1_V710_N  1104  1078   1  442998800  60168832      6   \n",
       "1870   FATB      P  R1_V710_N  1104  1078   1  442998800  60368832      6   \n",
       "1871     >>      X  R1_V710_N  1104     1   1  442998800     35132  56748   \n",
       "1872     ^^      X  R1_V710_N     1  1078   1  442998800     35132  56748   \n",
       "1873     !!      X  R1_V710_N     3   175   1          0     35132  56748   \n",
       "\n",
       "      ip3  ...  nbits  grtyp    ig1    ig2    ig3    ig4      datev  \\\n",
       "0       0  ...     16      Z  33792  77761      1      0  443004200   \n",
       "1       0  ...     16      Z  33792  77761      1      0  443004200   \n",
       "2       0  ...     16      Z  33792  77761      1      0  443004200   \n",
       "3       0  ...     16      Z  33792  77761      1      0  443004200   \n",
       "4       0  ...     16      Z  33792  77761      1      0  443004200   \n",
       "...   ...  ...    ...    ...    ...    ...    ...    ...        ...   \n",
       "1869    0  ...     12      Z  35132  56748      1      0  443004200   \n",
       "1870    0  ...     12      Z  35132  56748      1      0  443004200   \n",
       "1871    1  ...     32      E   1470    560  54400  46560  442998800   \n",
       "1872    1  ...     32      E   1470    560  54400  46560  442998800   \n",
       "1873    0  ...     64      X   5005      0    300   1500          0   \n",
       "\n",
       "            grid  date_of_observation  date_of_observation_Canada_Eastern  \n",
       "0     3379277761  2020-07-14 12:00:00                 2020-07-14 08:00:00  \n",
       "1     3379277761  2020-07-14 12:00:00                 2020-07-14 08:00:00  \n",
       "2     3379277761  2020-07-14 12:00:00                 2020-07-14 08:00:00  \n",
       "3     3379277761  2020-07-14 12:00:00                 2020-07-14 08:00:00  \n",
       "4     3379277761  2020-07-14 12:00:00                 2020-07-14 08:00:00  \n",
       "...          ...                  ...                                 ...  \n",
       "1869  3513256748  2020-07-14 12:00:00                 2020-07-14 08:00:00  \n",
       "1870  3513256748  2020-07-14 12:00:00                 2020-07-14 08:00:00  \n",
       "1871  3513256748  2020-07-14 12:00:00                 2020-07-14 08:00:00  \n",
       "1872  3513256748  2020-07-14 12:00:00                 2020-07-14 08:00:00  \n",
       "1873  3513256748                  NaT                                 NaT  \n",
       "\n",
       "[1874 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "df = fstpy.StandardFileReader(\"/fs/site4/eccc/cmd/w/sbf000/source_data_5005.std\").to_pandas()\n",
    "df = fstpy.add_columns(df, [\"dateo\"])\n",
    "df.drop(columns=[\"d\"])\n",
    "new_df = fstpy.add_timezone_column(df, \"date_of_observation\", \"Canada/Eastern\")\n",
    "new_df.drop(columns=[\"d\"])\n",
    "new_df.drop(columns=[\"d\"])  # .loc[new_df.date_of_validity_Canada_Eastern==datetime.datetime(2020,7,14,14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hu_df = fstpy.StandardFileReader(\"/home/sbf000/ss5/source_data_5005.std\", decode_metadata=True).to_pandas()\n",
    "hu_df = hu_df.loc[~(hu_df.nomvar == \"HU\")].reset_index(drop=True)\n",
    "hu_df.drop(\"d\", axis=1)\n",
    "hu_df.level.unique().size\n",
    "print(hu_df[\"level\"].unique())\n",
    "print(len(list(hu_df.level.unique())))\n",
    "hu_df\n",
    "# print(hu_df.loc[hu_df.nomvar==\"HU\"].drop('d', axis=1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = fstpy.StandardFileReader(\"/fs/site3/eccc/cmd/w/sbf000/spooki_tmpdir/test_9.std\").to_pandas()\n",
    "res_df[\"ip3\"] = 41094464\n",
    "res_df\n",
    "if os.path.isfile(\"/home/sbf000/test.toto\"):\n",
    "    os.remove(\"/home/sbf000/test.toto\")\n",
    "fstpy.StandardFileWriter(\"/home/sbf000/test.toto\", res_df).to_fst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hu_df1 = fstpy.StandardFileReader(\"/home/sbf000/ss5/source_data_5005.std\", decode_metadata=True).to_pandas()\n",
    "hu_df1 = hu_df1.loc[~(hu_df1.nomvar == \"HU\")].reset_index(drop=True)\n",
    "print(hu_df1[\"level\"].unique())\n",
    "print(len(list(hu_df1.level.unique())))\n",
    "print(hu_df1.loc[hu_df1.level == 200])\n",
    "hu_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fstpy\n",
    "\n",
    "res_df = fstpy.StandardFileReader(\"/fs/site3/eccc/cmd/w/sbf000/spooki_tmpdir/test_9.std\").to_pandas()\n",
    "cmp_df = fstpy.StandardFileReader(\n",
    "    \"/fs/site4/eccc/cmd/w/spst900/spooki/spooki_dir/pluginsRelatedStuff/GridPointDistance/testsFiles/GlbPres_test9_file2cmp.std\"\n",
    ").to_pandas()\n",
    "arr1 = res_df.loc[res_df.nomvar == \"GDX\"].iloc[0].d.compute()\n",
    "arr2 = cmp_df.loc[cmp_df.nomvar == \"GDX\"].iloc[0].d.compute()\n",
    "# print(arr1)\n",
    "# print(arr2)\n",
    "for i in range(arr1.shape[0]):\n",
    "    if not (arr1[i] == arr2[i]).all():\n",
    "        print(i, arr1[i])\n",
    "        print(i, arr2[i])\n",
    "    # print(i,arr1[i]==arr2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fstpy\n",
    "import copy\n",
    "import glob\n",
    "import rpnpy.librmn.all as rmn\n",
    "import rpnpy.vgd.all as vgd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "import numpy.ma as ma\n",
    "import fstd2nc\n",
    "import xarray as xr\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# '/fs/site4/eccc/cmd/w/spst900/spooki/spooki_dir/pluginsRelatedStuff'\n",
    "# fstpy.fstpy_log_level_debug()\n",
    "# fstpy.setup_fstpy_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"/fs/site4/eccc/cmd/w/spst900/spooki/spooki_dir/pluginsRelatedStuff/Pressure/**\", recursive=True)\n",
    "with mp.Pool(processes=40) as pool:\n",
    "    res = pool.map(fstpy.maybeFST, [file for file in files])  # runs in *only* one process\n",
    "\n",
    "files = list(ma.masked_array(files, mask=[not elem for elem in res]))\n",
    "\n",
    "files = [f for f in files if f != \"masked\"]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(processes=40) as pool:\n",
    "    df_list = pool.map(fstpy.get_basic_dataframe, [file for file in files])\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fstpy\n",
    "# import dask.array as da\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "df = fstpy.get_basic_dataframe(\"/home/sbf000/ss5/source_data_5005.std\")\n",
    "df = fstpy.add_grid_column(df)\n",
    "df = fstpy.add_columns(df, \"ip_info\")\n",
    "tt_df = df.loc[df.nomvar == \"TT\"]\n",
    "tt_df = tt_df.sort_values(by=\"level\", ascending=False)\n",
    "tt_df = fstpy.add_dask_column(tt_df)\n",
    "tt_df = fstpy.compute(tt_df)\n",
    "# for i in tt_df.index:\n",
    "#     tt_df.at[i,'d'] = tt_df.at[i,'d'].flatten()\n",
    "\n",
    "array_3d = np.stack(tt_df.d)\n",
    "\n",
    "# columns = array_3d.T\n",
    "# data = np.split(columns, columns.shape[0])\n",
    "# tt_df.drop(['key','path','shape','ip1_kind','ip1_pkind', 'ip2_dec', 'ip2_kind', 'ip2_pkind', 'ip3_dec', 'ip3_kind','ip3_pkind', 'surface', 'follow_topography', 'ascending', 'interval','vctype'], inplace=True, errors='ignore')\n",
    "# new_df = pd.DataFrame([tt_df.iloc[0].to_dict() for i in range(columns.shape[0])])\n",
    "\n",
    "# new_df['d'] = data\n",
    "# new_df.drop('d', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fstd2nc\n",
    "\n",
    "ds = fstd2nc.Buffer(\"/home/sbf000/ss5/source_data_5005.std\", vars=(\"TT\", \"P0\", \"HU\")).to_xarray()\n",
    "ds = ds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_zarr(\"weather.zarr\", compute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds_zarr = xr.open_zarr(\"weather.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zarr.TT.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_df = fstpy.QuickPressure(df).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_df.drop(\"d\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\n",
    "    \"/fs/site4/eccc/cmd/w/spst900/spooki/spooki_dir/pluginsRelatedStuff/GridPointDifference/**\", recursive=True\n",
    ")\n",
    "with mp.Pool(processes=40) as pool:\n",
    "    res = pool.map(fstpy.maybeFST, [file for file in files])  # runs in *only* one process\n",
    "\n",
    "files = list(ma.masked_array(files, mask=[not elem for elem in res]))\n",
    "\n",
    "files = [f for f in files if f != \"masked\"]\n",
    "\n",
    "df = pd.concat([fstpy.get_basic_dataframe(f) for f in files], ignore_index=True)\n",
    "path_groups = df.groupby(\"path\")\n",
    "for path, path_df in path_groups:\n",
    "    print(path)\n",
    "    print(path_df.drop([\"path\", \"key\"], axis=1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETIKET = \"GPTDIS\"\n",
    "files = glob.glob(\n",
    "    \"/fs/site4/eccc/cmd/w/spst900/spooki/spooki_dir/pluginsRelatedStuff/GridPointDistance/**\", recursive=True\n",
    ")\n",
    "with mp.Pool(processes=40) as pool:\n",
    "    res = pool.map(fstpy.maybeFST, [file for file in files])  # runs in *only* one process\n",
    "\n",
    "files = list(ma.masked_array(files, mask=[not elem for elem in res]))\n",
    "\n",
    "files = [f for f in files if f != \"masked\"]\n",
    "\n",
    "df = fstpy.StandardFileReader(files).to_pandas()\n",
    "# df = fstpy.StandardFileReader('/fs/site4/eccc/cmd/w/spst900/spooki/spooki_dir/pluginsRelatedStuff/GridPointDistance/testsFiles/2015072100_240_TTESUUVV_YinYang.std').to_pandas()\n",
    "# df = fstpy.StandardFileReader('/fs/site4/eccc/cmd/w/spst900/spooki/spooki_dir/pluginsRelatedStuff/GridPointDistance/testsFiles/ps5x4_fileSrc.std').to_pandas()\n",
    "# print(df.drop('d', axis=1).to_string())\n",
    "cmp_df = fstpy.StandardFileReader(\n",
    "    \"/fs/site4/eccc/cmd/w/spst900/spooki/spooki_dir/pluginsRelatedStuff/GridPointDistance/testsFiles/XCentered_file2cmp_rmn19.std\"\n",
    ").to_pandas()\n",
    "df = fstpy.add_path_and_key_columns(df)\n",
    "\n",
    "VEZCALCDIST_F = np.vectorize(rmn.ezcalcdist)\n",
    "\n",
    "\n",
    "def centered_difference(lats: np.ndarray, lons: np.ndarray) -> np.ndarray:\n",
    "    fres = VEZCALCDIST_F(lats[1], lons[1], lats[0], lons[0])\n",
    "    cres = VEZCALCDIST_F(lats[2:], lons[2:], lats[:-2], lons[:-2])\n",
    "    lres = VEZCALCDIST_F(lats[-1], lons[-1], lats[-2], lons[-2])\n",
    "    res = np.vstack([fres, cres, lres]).astype(np.float32)\n",
    "    return res\n",
    "\n",
    "\n",
    "def forward_difference(lats: np.ndarray, lons: np.ndarray) -> np.ndarray:\n",
    "    fres = VEZCALCDIST_F(lats[1:], lons[1:], lats[0:-1], lons[0:-1])\n",
    "    lres = VEZCALCDIST_F(lats[-1], lons[-1], lats[-2], lons[-2])\n",
    "    res = np.vstack([fres, lres]).astype(np.float32)\n",
    "    return res\n",
    "\n",
    "\n",
    "def backward_difference(lats: np.ndarray, lons: np.ndarray) -> np.ndarray:\n",
    "    fres = VEZCALCDIST_F(lats[1], lons[1], lats[0], lons[0])\n",
    "    cres = VEZCALCDIST_F(lats[1:], lons[1:], lats[0:-1], lons[0:-1])\n",
    "    res = np.vstack([fres, cres]).astype(np.float32)\n",
    "    return res\n",
    "\n",
    "\n",
    "TYPE = \"centered\"\n",
    "AXIS = [\"x\"]\n",
    "# TYPE = 'forward'\n",
    "# TYPE = 'backward'\n",
    "\n",
    "path_groups = df.groupby(\"path\")\n",
    "for path, path_df in path_groups:\n",
    "    print(path)\n",
    "\n",
    "    # file_id = rmn.fstopenall(path, rmn.FST_RO)\n",
    "    grid_groups = path_df.groupby([\"grid\"])\n",
    "    for grid, grid_df in grid_groups:\n",
    "        no_meta_df = grid_df.loc[~grid_df.nomvar.isin([\"^^\", \">>\", \"^>\", \"!!\", \"!!SF\", \"HY\", \"P0\", \"PT\"])].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        if no_meta_df.empty:\n",
    "            continue\n",
    "\n",
    "        df_list = []\n",
    "        grtyp_groups = no_meta_df.groupby(\"grtyp\")\n",
    "        for grtyp, grtyp_df in grtyp_groups:\n",
    "            print(f\"grtyp: {grtyp}\")\n",
    "            if grtyp == \"X\":\n",
    "                logging.warning(f\"{grtyp} is an unsupported grid type!\")\n",
    "                continue\n",
    "\n",
    "            model_df = pd.DataFrame([grtyp_df.iloc[0].to_dict()])\n",
    "            model_df[\"etiket\"] = ETIKET\n",
    "            model_df[\"ip1\"] = 0\n",
    "            model_df[\"nomvar\"] = \"GDX\"\n",
    "            gdx_df = copy.deepcopy(model_df)\n",
    "            model_df[\"nomvar\"] = \"GDY\"\n",
    "            gdy_df = copy.deepcopy(model_df)\n",
    "\n",
    "            if grtyp_df.empty:\n",
    "                continue\n",
    "            # print(no_meta_df.columns)\n",
    "            grid_params = fstpy.get_grid_definition_params(grtyp_df)\n",
    "            (lat, lon) = fstpy.get_2d_lan_lon_arr(grid_params)\n",
    "\n",
    "            print(fstpy.is_global_grid(grid_params, lon))\n",
    "#             # print(lat.shape,lon.shape)\n",
    "#             if TYPE == 'centered':\n",
    "#                 if 'x' in AXIS:\n",
    "#                     gdx_df['d'] = [centered_difference(lat,lon)]\n",
    "#                     df_list.append(gdx_df)\n",
    "#                 if 'y' in AXIS:\n",
    "#                     gdy_df['d'] = [centered_difference(lat.T,lon.T).T]\n",
    "#                     df_list.append(gdy_df)\n",
    "\n",
    "#             elif TYPE == 'forward':\n",
    "#                 if 'x' in AXIS:\n",
    "#                     gdx_df['d'] = [forward_difference(lat,lon)]\n",
    "#                     df_list.append(gdx_df)\n",
    "#                 if 'y' in AXIS:\n",
    "#                     gdy_df['d'] = [forward_difference(lat.T,lon.T).T]\n",
    "#                     df_list.append(gdy_df)\n",
    "#             elif TYPE == 'backward':\n",
    "#                 if 'x' in AXIS:\n",
    "#                     gdx_df['d'] = [backward_difference(lat,lon)]\n",
    "#                     df_list.append(gdx_df)\n",
    "\n",
    "#                 if 'y' in AXIS:\n",
    "#                     gdy_df['d'] = [backward_difference(lat.T,lon.T).T]\n",
    "#                     df_list.append(gdy_df)\n",
    "\n",
    "\n",
    "#             res_df = pd.concat(df_list, ignore_index=True)\n",
    "#             print(res_df.drop(['d','path','key'], axis=1).to_string())\n",
    "#             # [print(row.d) for row in res_df.itertuples()]\n",
    "\n",
    "#             print('********************************************************************************')\n",
    "\n",
    "\n",
    "# print(cmp_df.drop('d', axis=1).to_string())\n",
    "# if 'x' in AXIS:\n",
    "#     print(cmp_df.loc[cmp_df.nomvar=='GDX'].iloc[0].d.compute())\n",
    "# if 'y' in AXIS:\n",
    "#     print(cmp_df.loc[cmp_df.nomvar=='GDY'].iloc[0].d.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(arr1.shape[0]):\n",
    "\n",
    "#     if not (arr1[i].astype(np.int32)==arr2[i].astype(np.int32)).all():\n",
    "#         print(i)\n",
    "#         print(arr1[i])\n",
    "#         print(arr2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = list(df.columns)\n",
    "# cols.remove('key')\n",
    "# cols.remove('path')\n",
    "# cols.remove('shape')\n",
    "# if 'grid' not in df.columns:\n",
    "#     df = fstpy.add_grid_column(df)\n",
    "# if 'ip1_kind' not in df.columns:\n",
    "#     df = fstpy.add_columns(df,'ip_info')\n",
    "# if 'path' not in df.columns:\n",
    "#     df = fstpy.add_path_and_key_columns(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Pressure2Error(Exception):\n",
    "#     pass\n",
    "\n",
    "# class Pressure2:\n",
    "\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df\n",
    "#         self.validate_input()\n",
    "\n",
    "#     def validate_input(self):\n",
    "#         if self.df.empty:\n",
    "#             raise Pressure2Error('No data to process')\n",
    "\n",
    "#         self.meta_df = self.df.loc[self.df.nomvar.isin(\n",
    "#             [\"^^\", \">>\", \"^>\", \"!!\", \"!!SF\", \"HY\", \"P0\", \"PT\"])].reset_index(drop=True)\n",
    "#         if 'grid' not in self.df.columns:\n",
    "#             self.df = fstpy.add_grid_column(self.df)\n",
    "#         if 'ip1_kind' not in self.df.columns:\n",
    "#             self.df = fstpy.add_columns(self.df,'ip_info')\n",
    "#         if 'path' not in self.df.columns:\n",
    "#             self.df = fstpy.add_path_and_key_columns(self.df)\n",
    "\n",
    "#     def compute(self):\n",
    "#         df = self.df\n",
    "#         grid_groups = df.groupby(['path'])\n",
    "#         df_list = []\n",
    "#         for path, path_df in grid_groups:\n",
    "#             # file_hy_df = path_df.loc[path_df.nomvar.isin([\"HY\"])].reset_index(drop=True)\n",
    "#             grid_groups = path_df.groupby(['grid'])\n",
    "\n",
    "#             for _, grid_df in grid_groups:\n",
    "#                 grids_meta_df = grid_df.loc[grid_df.nomvar.isin([\"!!\", \"P0\", \"PT\",\"!!SF\"])].reset_index(drop=True)\n",
    "\n",
    "#                 vctypes_groups = grid_df.groupby(['vctype'])\n",
    "#                 for vctype, vt_df in vctypes_groups:\n",
    "#                     if vctype == fstpy.VerticalCoordType.UNKNOWN:\n",
    "#                         continue\n",
    "\n",
    "#                     datev_groups = vt_df.groupby(['datev'])\n",
    "#                     for _, dv_df in datev_groups:\n",
    "#                         without_meta_df = dv_df.loc[(dv_df.ip1 != 0) & (~dv_df.nomvar.isin([\"!!\", \"HY\", \"P0\", \"PT\", \">>\", \"^^\", \"PN\", \"PX\", \"PXSA\"]))]\n",
    "\n",
    "#                         if without_meta_df.empty:\n",
    "#                             continue\n",
    "#                         else:\n",
    "#                             levels = list(without_meta_df.level)\n",
    "#                             ips = list(without_meta_df.ip1)\n",
    "#                             levels_df = pd.DataFrame({'ip1':ips,'level':levels})\n",
    "#                             levels_df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "#                             print('---------------------------------------------------------------')\n",
    "#                             print(path)\n",
    "#                             vcoord = fstpy.get_vertical_coord(path_df, grids_meta_df, without_meta_df)\n",
    "#                             print(vcoord)\n",
    "#                             px_df = vcoord.pressure()\n",
    "#                             if not px_df.empty:\n",
    "#                                 print(px_df[['nomvar','typvar','etiket','ni','nj','nk','dateo','ip1','ip2','ip3','deet','npas','grtyp','datyp','nbits','ig1','ig2','ig3','ig4','unit']].head().to_string())\n",
    "#         #                         df_list.append(px_df)\n",
    "#         #     df_list.append(self.meta_df)\n",
    "#         # return final_results(df_list, Pressure2Error, self.meta_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_meta_fields(grid):\n",
    "#     toctoc = grid.loc[grid.nomvar == \"!!\"]\n",
    "#     vcode = list(toctoc.ig1.unique()) if not toctoc.empty else [-1]\n",
    "#     # vcode = []\n",
    "#     # if not toctoc.empty:\n",
    "#     #     vcode = list(toctoc.ig1.unique())\n",
    "#     #     # for row in toctoc.itertuples():\n",
    "#     #     #     vcode.append(row.ig1)\n",
    "\n",
    "#     # else:\n",
    "#     #     vcode.append(-1)\n",
    "\n",
    "#     p0 = grid.loc[grid.nomvar==\"P0\"]\n",
    "#     e1 = grid.loc[grid.nomvar==\"E1\"]\n",
    "#     pt = grid.loc[grid.nomvar==\"PT\"]\n",
    "#     hy = grid.loc[grid.nomvar==\"HY\"]\n",
    "#     sf = grid.loc[grid.nomvar==\"!!SF\"]\n",
    "#     return toctoc,(not toctoc.empty), p0,(not p0.empty), e1,(not e1.empty), pt,(not pt.empty), hy,(not hy.empty), sf,(not sf.empty), vcode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A partir d'ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vcoord_info = {\n",
    "#     'PRESSURE_2001':{\n",
    "#         'kind':2,\n",
    "#         'version':1,\n",
    "#         'vcode':2001,\n",
    "#         'fields':[],\n",
    "#         'eq':'pres = level'\n",
    "#         },\n",
    "#     'HYBRID_5002':{\n",
    "#         'kind':5,\n",
    "#         'version':2,\n",
    "#         'vcode':5002,\n",
    "#         'fields':['P0','!!'],\n",
    "#         'eq':'pres = np.exp(A+B*np.log(P0*100./pref)) / 100.0'\n",
    "#         },\n",
    "#     'HYBRID_5003':{\n",
    "#         'kind':5,\n",
    "#         'version':3,\n",
    "#         'vcode':5003,\n",
    "#         'fields':['P0','!!'],\n",
    "#         'eq':'pres = np.exp(A+B*np.log(P0*100./pref)) / 100.0'\n",
    "#         },\n",
    "#     'HYBRID_5004':{\n",
    "#         'kind':5,\n",
    "#         'version':4,\n",
    "#         'vcode':5004,\n",
    "#         'fields':['P0','!!'],\n",
    "#         'eq':'pres = np.exp(A+B*np.log(P0*100./pref)) / 100.0'\n",
    "#         },\n",
    "#     'HYBRID_5005':{\n",
    "#         'kind':5,\n",
    "#         'version':5,\n",
    "#         'vcode':5005,\n",
    "#         'fields':['P0','!!'],\n",
    "#         'eq':'pres = np.exp(A+B*np.log(P0*100./pref)) / 100.0'\n",
    "#         },\n",
    "#     'ETA_1002':{\n",
    "#         'kind':1,\n",
    "#         'version':2,\n",
    "#         'vcode':1002,\n",
    "#         'fields':['P0','PT'],\n",
    "#         'eq':'pres = lvl * (P0-PT) + PT'\n",
    "#         },\n",
    "#     'SIGMA_1001':{\n",
    "#         'kind':1,\n",
    "#         'version':1,\n",
    "#         'vcode':1001,\n",
    "#         'fields':['P0'],\n",
    "#         'eq':'pres = lvl * P0'\n",
    "#         },\n",
    "#     'HYBRID_5001':{\n",
    "#         'kind':5,\n",
    "#         'version':1,\n",
    "#         'vcode':5001,\n",
    "#         'fields':['P0','HY'],\n",
    "#         'eq':'pres = (A + B * P0)'\n",
    "#         }\n",
    "#     }\n",
    "# def pressure_pres(shape,levels):\n",
    "#     pres = [da.full(shape, lvl, dtype=np.float32,order='F') for lvl in levels]\n",
    "#     return pres\n",
    "\n",
    "# def sigma_pres(p0_df,levels):\n",
    "#     if 'd' not in p0_df.columns:\n",
    "#         p0_df = fstpy.add_dask_column(p0_df)\n",
    "#     p0 = p0_df.iloc[0].d\n",
    "#     pres = [lvl * p0 for lvl in levels]\n",
    "#     return pres\n",
    "\n",
    "# def eta_pres(pt_df, p0_df, levels):\n",
    "#     if 'd' not in pt_df.columns:\n",
    "#         pt_df = fstpy.add_dask_column(pt_df)\n",
    "#     pt_df = fstpy.compute(pt_df)\n",
    "#     if 'd' not in p0_df.columns:\n",
    "#         p0_df = fstpy.add_dask_column(p0_df)\n",
    "#     p0 = p0_df.iloc[0].d\n",
    "#     pt = pt_df.iloc[0].d\n",
    "#     pres = [lvl * (p0-pt) + pt for lvl in levels]\n",
    "#     return pres\n",
    "\n",
    "# def hybrid_pres(file_hy_df, p0_df, levels):\n",
    "#     if 'd' not in file_hy_df.columns:\n",
    "#         file_hy_df = fstpy.add_dask_column(file_hy_df)\n",
    "#     hy_df = fstpy.compute(file_hy_df)\n",
    "#     if 'd' not in p0_df.columns:\n",
    "#         p0_df = fstpy.add_dask_column(p0_df)\n",
    "#     p0 = p0_df.iloc[0].d\n",
    "#     ptop = hy_df.iloc[0]['d'][0]\n",
    "#     pref = hy_df.iloc[0]['ig1']\n",
    "#     rcoef = hy_df.iloc[0]['ig2'] / 1000.0\n",
    "#     # print(ptop,pref,rcoef)\n",
    "#     etatop = ptop/pref\n",
    "#     B = ((levels - etatop) / (1 - etatop)) ** rcoef\n",
    "#     A = pref * (levels - B)\n",
    "#     vcoord_table = pd.DataFrame({'lvl': levels, 'A':A, 'B':B})\n",
    "#     vcoord_table.drop_duplicates(inplace=True, ignore_index=True)\n",
    "#     # print(vcoord_table)\n",
    "#     pres = [(vcoord_table.at[idx,'A'] + vcoord_table.at[idx,'B'] * p0) for idx in vcoord_table.index]\n",
    "#     return pres\n",
    "\n",
    "# def hybstag_pres(toctoc_df, p0_df, levels_df) -> list:\n",
    "#     if 'd' not in toctoc_df.columns:\n",
    "#         toctoc_df = fstpy.add_dask_column(toctoc_df)\n",
    "#     toctoc_df = fstpy.compute(toctoc_df)\n",
    "#     if 'd' not in p0_df.columns:\n",
    "#         p0_df = fstpy.add_dask_column(p0_df)\n",
    "\n",
    "#     # print('computing pressure')\n",
    "#     toctoc = toctoc_df.iloc[0]['d']\n",
    "#     pref = toctoc[1][1]\n",
    "#     vcoord_table = pd.DataFrame({'ip1':toctoc[0].astype(np.int32),'A':toctoc[1],'B':toctoc[2]})\n",
    "#     vcoord_table = pd.merge(vcoord_table, levels_df, on=\"ip1\")\n",
    "#     vcoord_table.drop_duplicates(inplace=True, ignore_index=True)\n",
    "#     # print(vcoord_table)\n",
    "#     p0 = p0_df.iloc[0].d\n",
    "#     s = np.log(p0*100./pref)\n",
    "#     pres = [(np.exp(vcoord_table.at[idx,'A'] + vcoord_table.at[idx,'B'] * s) / 100.0) for idx in vcoord_table.index]\n",
    "#     return pres\n",
    "\n",
    "# def pressure(df):\n",
    "#     grid_groups = df.groupby(['path'])\n",
    "\n",
    "#     for path, path_df in grid_groups:\n",
    "#         file_hy_df = path_df.loc[path_df.nomvar.isin([\"HY\"])].reset_index(drop=True)\n",
    "#         grid_groups = path_df.groupby(['grid'])\n",
    "\n",
    "#         for _, grid_df in grid_groups:\n",
    "#             grids_meta_df = grid_df.loc[grid_df.nomvar.isin([\"!!\", \"P0\", \"PT\",\"!!SF\"])].reset_index(drop=True)\n",
    "\n",
    "#             vctypes_groups = grid_df.groupby(['vctype'])\n",
    "#             for vctype, vt_df in vctypes_groups:\n",
    "#                 if vctype == fstpy.VerticalCoordType.UNKNOWN:\n",
    "#                     continue\n",
    "\n",
    "#                 datev_groups = vt_df.groupby(['datev'])\n",
    "#                 for datev, dv_df in datev_groups:\n",
    "#                     without_meta_df = dv_df.loc[(dv_df.ip1 != 0) & (~dv_df.nomvar.isin([\"!!\", \"HY\", \"P0\", \"PT\", \">>\", \"^^\", \"PN\", \"PX\", \"PXSA\"]))]\n",
    "\n",
    "#                     if without_meta_df.empty:\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         # base_dict = without_meta_df[['nomvar','typvar','etiket','ni','nj','nk','dateo','ip1','ip2','ip3','deet','npas','grtyp','datyp','nbits','ig1','ig2','ig3','ig4']].iloc[0].to_dict()\n",
    "#                         # print(base_dict)\n",
    "\n",
    "#                         # for col in ['nomvar','typvar','etiket','ni','nj','nk','dateo','ip1','ip2','ip3','deet','npas','grtyp','datyp','nbits','ig1','ig2','ig3','ig4']:\n",
    "#                         #     values = without_meta_df[col].unique()\n",
    "#                         #     print(f'{col}: {values}')\n",
    "#                         levels = list(without_meta_df.level)\n",
    "#                         ips = list(without_meta_df.ip1)\n",
    "#                         levels_df = pd.DataFrame({'ip1':ips,'level':levels})\n",
    "#                         levels_df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "#                         # container\n",
    "\n",
    "#                         print('---------------------------------------------------------------')\n",
    "#                         print(path)\n",
    "#                         vcoord = get_vertical_coord(path_df, grids_meta_df, without_meta_df)\n",
    "#                         print(type(vcoord))\n",
    "#                         px_df = vcoord.pressure()\n",
    "#                         if not px_df.empty:\n",
    "#                             print(px_df[['nomvar','typvar','etiket','ni','nj','nk','dateo','ip1','ip2','ip3','deet','npas','grtyp','datyp','nbits','ig1','ig2','ig3','ig4','unit']])\n",
    "# print(vcoord)\n",
    "# print(f'ip1:{ip1}, ip2:{ip2}')\n",
    "#     if vctype=='PRESSURE_2001':\n",
    "\n",
    "#         # print(vcoord_info[vctype])\n",
    "#         print(vctype)\n",
    "#         if 'd' not in without_meta_df.columns:\n",
    "#             a_df = pd.DataFrame([without_meta_df.iloc[0].to_dict()])\n",
    "#             a_df = fstpy.add_dask_column(a_df)\n",
    "#         shape = a_df.iloc[0]['d'].shape\n",
    "#         pres = pressure_pres(shape,levels)\n",
    "#         # print(pres)\n",
    "\n",
    "\n",
    "#     elif vctype == 'HYBRID_5001':\n",
    "#         # print(vcoord_info[vctype])\n",
    "#         print(vctype)\n",
    "#         p0_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"P0\") & (grids_meta_df.datev==datev)]\n",
    "#         if file_hy_df.empty or p0_df.empty:\n",
    "#             print('P0 or !HY missing')\n",
    "#             continue\n",
    "#         else:\n",
    "#             pres = hybrid_pres(file_hy_df, p0_df, levels)\n",
    "#             # print(pres)\n",
    "\n",
    "#     elif vctype == 'HYBRID_5002':\n",
    "#         # print(vcoord_info[vctype])\n",
    "#         print(vctype)\n",
    "#         p0_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"P0\") & (grids_meta_df.datev==datev)]\n",
    "#         toctoc_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"!!\") & (grids_meta_df.ig1==5002)]\n",
    "#         if toctoc_df.empty or p0_df.empty:\n",
    "#             print('P0 or !! missing')\n",
    "#             continue\n",
    "#         else:\n",
    "#             pres = hybstag_pres(toctoc_df, p0_df, levels_df)\n",
    "#             # print(pres)\n",
    "\n",
    "#     elif vctype == 'HYBRID_5003':\n",
    "#         # print(vcoord_info[vctype])\n",
    "#         print(vctype)\n",
    "#         p0_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"P0\") & (grids_meta_df.datev==datev)]\n",
    "#         toctoc_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"!!\") & (grids_meta_df.ig1==5003)]\n",
    "#         if toctoc_df.empty or p0_df.empty:\n",
    "#             print('P0 or !! missing')\n",
    "#             continue\n",
    "#         else:\n",
    "#             pres = hybstag_pres(toctoc_df, p0_df, levels_df)\n",
    "#             # print(pres)\n",
    "\n",
    "#     elif vctype == 'HYBRID_5004':\n",
    "#         # print(vcoord_info[vctype])\n",
    "#         print(vctype)\n",
    "#         p0_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"P0\") & (grids_meta_df.datev==datev)]\n",
    "#         toctoc_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"!!\") & (grids_meta_df.ig1==5004)]\n",
    "#         if toctoc_df.empty or p0_df.empty:\n",
    "#             print('P0 or !! missing')\n",
    "#             continue\n",
    "#         else:\n",
    "#             pres = hybstag_pres(toctoc_df, p0_df, levels_df)\n",
    "#             # print(pres)\n",
    "\n",
    "#     elif vctype == 'HYBRID_5005':\n",
    "#         # print(vcoord_info[vctype])\n",
    "#         print(vctype)\n",
    "#         p0_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"P0\") & (grids_meta_df.datev==datev)]\n",
    "#         toctoc_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"!!\") & (grids_meta_df.ig1==5005)]\n",
    "#         if toctoc_df.empty or p0_df.empty:\n",
    "#             print('P0 or !! missing')\n",
    "#             continue\n",
    "#         else:\n",
    "#             pres = hybstag_pres(toctoc_df, p0_df, levels_df)\n",
    "#             # print(pres)\n",
    "\n",
    "#     elif vctype == 'ETA_1002':\n",
    "#         # print(vcoord_info[vctype])\n",
    "#         p0_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"P0\") & (grids_meta_df.datev==datev)]\n",
    "#         pt_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"PT\") & (grids_meta_df.datev==datev)]\n",
    "#         if pt_df.empty or p0_df.empty:\n",
    "#             print('P0 or PT missing')\n",
    "#             continue\n",
    "#         else:\n",
    "#             pres = eta_pres(pt_df, p0_df, levels)\n",
    "#             # print(pres)\n",
    "\n",
    "#     elif vctype == 'SIGMA_1001':\n",
    "#         # print(vcoord_info[vctype])\n",
    "#         print(vctype)\n",
    "#         p0_df = grids_meta_df.loc[(grids_meta_df.nomvar==\"P0\") & (grids_meta_df.datev==datev)]\n",
    "#         if p0_df.empty:\n",
    "#             print('P0 missing')\n",
    "#             continue\n",
    "#         else:\n",
    "#             pres = sigma_pres(p0_df,levels)\n",
    "#             # print(pres)\n",
    "#     else:\n",
    "#         continue\n",
    "# # print(without_meta_df[cols].head().to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (path,grid), grid_df in grid_groups:\n",
    "#     without_meta_df = grid_df.loc[~grid_df.nomvar.isin([\"!!\", \"HY\", \"P0\", \"PT\", \">>\", \"^^\", \"PN\", \"PX\", \"PXSA\"])]\n",
    "#     if without_meta_df.empty:\n",
    "#         continue\n",
    "#     ip1 = without_meta_df.ig1.unique()[0] if without_meta_df.ig1.unique()[0] != 0 else -1\n",
    "#     ip2 = without_meta_df.ig2.unique()[0] if without_meta_df.ig2.unique()[0] != 0 else -1\n",
    "#     unit = rmn.fstopenall(path)\n",
    "#     toctoc, t_ex, p0, p0_ex, e1, e1_ex, pt, pt_ex, hy, hy_ex, sf, sf_ex, vcode = get_meta_fields(grid_df)\n",
    "#     kind_and_version = [divmod(vc,1000) for vc in vcode]\n",
    "#     kind_and_version = [(kind,-1) if version==999 else (kind,version) for kind,version in kind_and_version]\n",
    "#     print(f'1:{kind_and_version}')\n",
    "#     kind_and_version_f = []\n",
    "#     for k,v in kind_and_version:\n",
    "#         if k == -1:\n",
    "#             search_df = grid_df.loc[grid_df.ip1_kind.isin([1,2,5])]\n",
    "#             ip1_kinds = list(search_df.ip1_kind.unique())\n",
    "#             for kind in ip1_kinds:\n",
    "#                 kind_and_version_f.append((kind,v))\n",
    "#         else:\n",
    "#             kind_and_version_f.append((k,v))\n",
    "#     print(f'2:{kind_and_version_f}')\n",
    "#     print(ip1,ip2)\n",
    "#     # this_vcode = vcode[0]\n",
    "#     # ip1_kind_groups = grid_df.groupby(grid_df.ip1_kind)\n",
    "#     # for ip1_kind, ip1_kind_df in ip1_kind_groups:\n",
    "#     #     # these ip1_kinds are not defined\n",
    "#     #     if ip1_kind not in [1,2,5]:\n",
    "#     #         continue\n",
    "#     #     without_meta_df = ip1_kind_df.loc[(~ip1_kind_df.ip1_kind.isin([-1, 3, 6])) & (~ip1_kind_df.nomvar.isin([\"!!\", \"HY\", \"P0\", \"PT\", \">>\", \"^^\", \"PN\", \"PX\", \"PXSA\"]))]\n",
    "#     #     if not without_meta_df.empty:\n",
    "#     #         ip1 = without_meta_df.ig1.unique()[0]\n",
    "#     #         ip2 = without_meta_df.ig2.unique()[0]\n",
    "#     #         if (ip1 == 0) and (ip2 == 0):\n",
    "#     #             ip1 = -1\n",
    "#     #             ip2 = -1\n",
    "#     #         kind,version = divmod(this_vcode,1000)\n",
    "#     #         if version == 999:\n",
    "#     #             version = -1\n",
    "#     #         # print(ip1_kind,type(version),path,ip1,ip2)\n",
    "#     #         try:\n",
    "#     #             myvgd = vgd.vgd_read(unit, ip1=ip1, ip2=ip2, kind=ip1_kind, version=version)\n",
    "#     #         except vgd.VGDError:\n",
    "#     #             try:\n",
    "#     #                 myvgd = vgd.vgd_read(unit, ip1=-1, ip2=-1, kind=ip1_kind, version=version)\n",
    "#     #             except vgd.VGDError:\n",
    "#     #                 print(ip1_kind,version,path)\n",
    "#     #                 print(grid_df.loc[grid_df.nomvar.isin([\"!!\", \"HY\", \"P0\", \"PT\", \">>\", \"^^\", \"PN\", \"PX\", \"PXSA\"])][['nomvar','typvar','ip1','ip2','ig1','ig2']])\n",
    "#     #                 print(grid_df.loc[(grid_df.ip1_kind == ip1_kind) & ~grid_df.nomvar.isin([\"!!\", \"HY\", \"P0\", \"PT\", \">>\", \"^^\", \"PN\", \"PX\", \"PXSA\"])][['nomvar','typvar','ip1','ip2','ig1','ig2']])\n",
    "\n",
    "#     rmn.fstcloseall(unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AddTocTocError(Exception):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hy_df = meta_df.loc[meta_df.nomvar=='HY']\n",
    "# for i in hy_df.index:\n",
    "#     print(hy_df.loc[i].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p0_df = meta_df.loc[meta_df.nomvar=='P0']\n",
    "# g = p0_df.groupby(['path'])\n",
    "\n",
    "# for path,r in g:\n",
    "#     print(path)\n",
    "#     print(r[cols].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgd.vgd_put_opt('ALLOW_SIGMA', vgd.VGD_ALLOW_SIGMA)\n",
    "\n",
    "\n",
    "# fstpy.fstpy_log_level_info()\n",
    "\n",
    "# for (path, grid, datev, ip1_kind, vctype), current_df in groups:\n",
    "#     if vctype=='UNKNOWN':\n",
    "#         continue\n",
    "\n",
    "#     # ip1_kinds = current_df.ip1_kind.unique()\n",
    "#     # if len(ip1_kinds) and (not (ip1_kinds[0] is None)):\n",
    "#     #     for ip1_kind in ip1_kinds:\n",
    "#             # current_df = current_df.loc[current_df.ip1_kind==ip1_kind]\n",
    "#     if current_df.empty:\n",
    "#         continue\n",
    "#     ip1 = current_df.ig1.unique()\n",
    "#     ip2 = current_df.ig2.unique()\n",
    "#     if (len(ip1) == 1) and (len(ip2) == 1):\n",
    "#         ip1 = ip1[0]\n",
    "#         ip2 = ip2[0]\n",
    "#     else:\n",
    "#         raise AddTocTocError('BROKEN')\n",
    "#         # continue\n",
    "#     print('------------------------------------------------------------------------------------------------------------------')\n",
    "#     print(f'path {path}\\ngrid {grid}\\ndatev {datev}\\nip1_kind {ip1_kind}')\n",
    "#     print(vctype)\n",
    "#     # sf_df = meta_df.loc[(meta_df.nomvar=='!!SF') & (meta_df.grid==grid) & (meta_df.path==path)]\n",
    "#     # if not sf_df.empty:\n",
    "#     #     kind = str(sf_df.iloc[0].ig1)[0]\n",
    "#     #     key = int(sf_df.iloc[0].key)\n",
    "#     #     # print(f'kind {kind}')\n",
    "#     #     if int(kind) == ip1_kind:\n",
    "#     #         funit = rmn.fstopenall(path)\n",
    "#     #         data = np.squeeze(rmn.fstluk(key)['d'])\n",
    "#     #         rmn.fstcloseall(funit)\n",
    "#     #         data = pd.DataFrame({'ip1': data[0],'A':data[1],'B':data[2]})\n",
    "#     #         print(fstpy.voir(sf_df))\n",
    "#     #         print(data)\n",
    "#     #     # continue\n",
    "\n",
    "#     # toctoc_df = meta_df.loc[(meta_df.nomvar=='!!') & (meta_df.grid==grid) & (meta_df.path==path)]\n",
    "#     # if not toctoc_df.empty:\n",
    "#     #     kind = str(toctoc_df.iloc[0].ig1)[0]\n",
    "#     #     key = int(toctoc_df.iloc[0].key)\n",
    "#     #     # print(f'kind {kind}')\n",
    "#     #     if int(kind) == ip1_kind:\n",
    "#     #         funit = rmn.fstopenall(path)\n",
    "#     #         data = np.squeeze(rmn.fstluk(key)['d'])\n",
    "#     #         rmn.fstcloseall(funit)\n",
    "#     #         data = pd.DataFrame({'ip1': data[0],'A':data[1],'B':data[2]})\n",
    "#     #         print(fstpy.voir(toctoc_df))\n",
    "#     #         print(data)\n",
    "#     #     # continue\n",
    "\n",
    "#     funit = rmn.fstopenall(path)\n",
    "#     try:\n",
    "#         hy_df = meta_df.loc[(meta_df.nomvar=='HY') & (meta_df.path==path)]\n",
    "#         if (not hy_df.empty) or (vctype=='ETA_1002') or (vctype=='SIGMA_1001'):\n",
    "#             myvgd = vgd.vgd_read(funit, ip1=-1, ip2=-1, kind=ip1_kind, version=-1)\n",
    "#         else:\n",
    "#             myvgd = vgd.vgd_read(funit, ip1=ip1, ip2=ip2, kind=ip1_kind, version=-1)\n",
    "#     except:\n",
    "#         print(f'There was a problem reading the VGridDescriptor for {vctype}')\n",
    "#         continue\n",
    "#     finally:\n",
    "#         rmn.fstcloseall(funit)\n",
    "\n",
    "#     keys = ['KIND', 'VERSION', 'NL_M', 'NL_T','DIPT','DIPW','DHM','DHT','DHW','PREF','PTOP','RC_1','RC_2','RC_3','RC_4','RFLD','RFLS','VTBL']\n",
    "\n",
    "#     vcoord={}\n",
    "#     for k in keys:\n",
    "#         try:\n",
    "#             v = vgd.vgd_get(myvgd,k)\n",
    "#             # print(f'{k} = {vgd.vgd_get(myvgd,k)}')\n",
    "#         except:\n",
    "#             pass\n",
    "#         if not isinstance(v,np.ndarray):\n",
    "#             if v != -9999.0:\n",
    "#                 vcoord[k] = v\n",
    "#         else:\n",
    "#             vcoord[k] = np.squeeze(v)\n",
    "#     if 'VTBL' in vcoord.keys():\n",
    "#         vcoord['VTBL'] = pd.DataFrame({'ip1': vcoord['VTBL'][0],'A':vcoord['VTBL'][1],'B':vcoord['VTBL'][2]})\n",
    "\n",
    "#     print(f'KIND: {vcoord[\"KIND\"]} VERSION: {vcoord[\"VERSION\"]}')\n",
    "#     # for k in vcoord.keys():\n",
    "#     #     print(f'{k}={vcoord[k]}')\n",
    "#     vgd.vgd_free(myvgd)\n",
    "\n",
    "#     if vcoord['KIND'] == 1:\n",
    "#         if vcoord['VERSION'] == 1:\n",
    "#             pass\n",
    "#         elif vcoord['VERSION'] == 2:\n",
    "#             pass\n",
    "#         elif vcoord['VERSION'] == 3:\n",
    "#             pass\n",
    "#     elif (vcoord['KIND'] == 2) and (vcoord['VERSION'] == 1):\n",
    "#         pass\n",
    "#     elif vcoord['KIND'] == 5:\n",
    "#         if vcoord['VERSION'] == 1: #HY\n",
    "#             pass\n",
    "#         if vcoord['VERSION'] == 2:\n",
    "#             pass\n",
    "#         elif vcoord['VERSION'] == 3:\n",
    "#             pass\n",
    "#         elif vcoord['VERSION'] == 4:\n",
    "#             pass\n",
    "#         elif vcoord['VERSION'] == 5:\n",
    "#             pass\n",
    "#         elif vcoord['VERSION'] == 100:\n",
    "#             pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgd.vgd_free(myvgd)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64d9907cdecca58650b9cb02e55152d2e4441c47f8a8acbd8205e8101d835148"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('surgepy_1.0.8_all': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
